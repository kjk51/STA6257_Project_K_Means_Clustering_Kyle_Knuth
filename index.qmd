---
title: "K-Means Clustering"
author: "Kyle Knuth"
date: 'today'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "K-Means Clustering"?

K-means clustering is a form of unsupervised learning that uses “k”
clusters with assigned data to create groups. The main goal of k-means
clustering is to classify data within clusters that are as similar as
possible while also keeping it out of clusters as dissimilar as possible
[@sinaga2020unsupervised]. There are several methods of finding the
optimal number of clusters such as the Elbow method, Silhouette method,
Gap statistic, Canopy algorithm, information criterion approach, and
information theoretic approach. The Silhouette method is used to
visually find the number of clusters by using the silhouette
coefficients (equation 1). The silhouette coefficients can be between -1
and 1 where a value of 1 indicates that the data is far from the
neighboring cluster, a value of 0 means the data is close to the
neighboring clusters and a value of -1 means that the data was assigned
to the wrong cluster. The Gap statistic (equation 2), finds the optimal
number of clusters by comparing the within cluster dispersion changes to
expected with a null dispersion [@tibshirani2001estimating].  The Elbow
method consists of plotting the sum of squared error versus “K” numbers
and wherever the slope kinks at indicates the optimal number of clusters
[@lee2022identifying]. As for the information criterion approach, it
uses criteria such as the Akaike information criterion and the Bayesian
information criterion to balance the likelihood increase with additional
parameters; whereas, the information theoretic approach uses
rate-distortion theory and jump statistic to find the correct number of
clusters [@kodinariya2013review]\]. The last method of determining the
optimal number of clusters is the Canopy Algorithm. The Canopy method is
the best method for large and complex data sets and enhances the fault
tolerance [@yuan2019research]. When it comes to smaller and less complex
datasets, the other methods are better. Once the amount of clusters has
been determined, the quality of those clusters can be assessed. The
Calinski-Harabasz criterion is used to determine the quality of the data
and a higher value indicates that the clusters are better separated
[@qabbaah2019using]. The original data set is partitioned into clusters
that have common traits and each cluster has a cluster center that
starts from seed points. Then the k-means clustering method calculates
the distance between each data point and each cluster center and takes
an nxd matrix and assigns the data randomly to those clusters
[@javadi2017classification]. The data points are clustered by Euclidean
distance of intensity values to the centroid, and a new centroid is then
calculated and the previous steps are repeated until the labels of the
clusters no longer change [@dubey2013infected]. The Euclidean distance
formula (equation 3) is used to compute the distance between the object
center and each centroid[@9320839]. One of the most important parts to
make k-means clustering more efficient is to find the most optimal
centroids as quickly as possible. A solution to this is to use principal
component analysis to estimate the initial centroids rather than random
selection [@zubair2022improved]. Basically this means that this process
is looped over and over until there is a convergence and there are
distinct clusters. 

$$s=b-a/{max(a,b)}$$

Equation 1.

$$Gap_n(k)=E^*_n(log(W_k))-log(W_k)$$

Equation 2.

$$d(p,q)=\sqrt{\sum^n_{i=1}(q_i-p_i)^2}$$

Equation 3.

### Applications of K-Means Clustering

There have been many useful applications of k-means clustering ranging
from color segmentation to identifying allergen severity by age group.
There has been research done on using color segmentation with k-means
clustering to identify the diseased parts of the fruit. This was
achieved by transforming the image from RGB to L\*a\*b\* and labeling
the pixels of the image via k-means clustering [@dubey2013infected].
Using this method, the diseased part of the fruit was picked out.
K-means clustering can also be used to cluster geographic locations.
Using an aquifer, k-means clustering was able to identify risk areas of
the aquifer without using fixed weights [@javadi2017classification].
When compared to data physically collected from the aquifer, it was
shown that the map produced from clustering was very accurate. Another
example using geography is clustering regions of Indonesia based on
trade and accommodation facilities. Regions of Indonesia were clustered
into 3 categories of high, medium, and low trade and accommodation
[@munawar2021cluster]. K-means clustering also has medical applications
such as predicting chemotherapy response to cancer, how skin changes as
you age, pregnancy problems, and how your allergen sensitivity changes
as you age. Since it doesn’t have the same limitations as traditional
methods, k-means clustering was chosen to predict chemotherapy response
and was able to create clusters that acted as biomarkers to determine
the treatment’s effectiveness [@nguyen2015prediction]. Based on
different attributes of the pores in one’s skin, k-means clustering was
able to create groups based on the detectability and size of the poor.
From this information as well as the mean age of each cluster, the
changes in skin were able to be inferred by the age group of each
cluster. Similarly, by using the data from immunoglobulin tests, k-means
clustering created groups based on age to explain how for certain
allergens the sensitivity actually decreased as age increased
[@zhao2023k]. Pregnancy problems can also be analyzed with this method.
K-means clustering was used to aid health professionals in making
decisions about interventions to reduce pregnancy problems [@9154694].
In terms of a more academic field, k-means clustering was also utilized
to assess academic performance. Students were categorized into different
levels based on performance which enables a more comprehensive view than
traditional grouping based on average scores [@oyelade2010application].
K-means can also be used for the tourism industry. By using a modified
k-means algorithm, a tourist’s preferences and behaviors can be
clustered so that the satisfaction and experience can be improved
[@ijgi13020040]. Another field that k-means is useful in is agriculture.
When using k-means clustering soil, clusters were generated that
improved the understanding of soil classification and spatial
representation of that soil to make more informed decisions[@7377608].As
a last example, our method was used to cluster counties together based
on similarity to each other from economic factors to health behaviors so
that county-level counterfactuals could be found
[@strutz2022determining]. 

## Variations of K-Means Clustering

Apart from the traditional method of k-means clustering there is also
constrained k-means clustering. Constrained k-means clustering is mainly
used to increase robustness of the model and to deal with outlier
clusters that are far from their true clusters in order to enforce
consistency [@bradley2000constrained]. Another variation of the
traditional method is the improved k-means clustering algorithm. The
improved algorithm uses two data structures to retain the cluster labels
and their distances while reducing the amount of repeated distance
calculations [@na2010research]. This improved algorithm improves speed
and efficiency while maintaining accuracy. The next variation is called
the filtering algorithm. The filtering algorithm is an optimized
implementation of Lloyd's k-means using a kd-tree structure, and
performs best in clustering by refining candidate center selection
through axis-aligned splitting hyperplanes and efficient trimming
[@kanungo2002efficient]. The allure of this variation is that it has
great efficiency, is simplistic, and is scalable to higher dimensions.
Genetic-based k-means (GBKM) clustering is another alternative offering
speed and accuracy. What makes this algorithm different from the
traditional one is that the “centroids are encoded by chromosomes rather
than random initial cluster centroids”[@chougule2015genetic]. GBKM also
follows some of the same ideas of natural selection such as selection,
crossover, and mutation. This modified k-means algorithm selects the
fittest points from the clusters to produce a new generation by
performing single point crossover and one-bit flip mutation until the
best results are found [@chougule2015genetic]. K-means clustering can
also be used in conjunction with other tools. An example is the use of
MapReduce from Hadoop to aid with the amount of data used. In this
process, two files are created where one contains the centroid values
and the other contains data point values and the mapper function is
coded to calculate the distance between each centroid and data point
then assigned to the centroid with the lowest distance
[@anchalia2013mapreduce]. A new iteration of map reduce will be
performed again until the centroid value is not updated. Lastly, there
is the extended k-means algorithm. Extended k-means takes the number of
elements within a cluster into consideration by sorting the clusters in
ascending order based on the number of elements in each cluster and
either recruits or drops elements from the first cluster [@6642738]. If
there are n elements in the cluster and an element is needed, the
difference, a-n, is calculated and points from other clusters are
recruited. 

## Limitations

Even though k-means clustering is a great tool for data analysis it
still has its limitations. A few of those limitations are the random
initialisations of centroids, the requirement to know in advance the
number of clusters present within a dataset, and also the inability of
the algorithm to handle a wide variety of very different types of data
[@electronics9081295]. If you were to choose too many or too few
clusters, a lot of variability will be brought into the model. If the
data were to become more complex, there would be issues in the
clustering. There needs to be an adequate sample size and the data needs
to be preprocessed very carefully due to its scalability. 

## Methods

## Analysis and Results

### Data and Visualization

A study was conducted to determine how...

### Statistical Modeling

### Conclusion

## References
