---
title: "Using K-Means Clustering to determine groupings of fatal car crashes"
authors:
  - name: Kyle Knuth
  - name: Siva Gopavarapu
  - name: Nikhitha Karlapudi
  - name: Eswar Sai Burugupalley
  - name: Bala Venkataprasad Sadhana Vittal
date: 'today'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

### What is "K-Means Clustering"?

K-means clustering is a form of unsupervised learning which uses “K”
clusters with assigned data to create groups. The main goal of K-means
clustering is to classify data within clusters, which are as similar as
possible, while also keeping it out of clusters as dissimilar as
possible [@sinaga2020unsupervised]. There are several methods of finding
the optimal number of clusters, such as the Elbow method, Silhouette
method, Gap statistic. The Silhouette method is used to visually find
the number of clusters by using the silhouette coefficients. The
silhouette coefficients can be between -1 and 1, where a value of 1
indicates the data is far from the neighboring cluster, a value of 0
means the data is close to the neighboring clusters and a value of -1
means the data was assigned to the wrong cluster. The Gap statistic
finds the optimal number of clusters by comparing the within cluster
dispersion changes to expected with a null dispersion
[@tibshirani2001estimating].  The Elbow method consists of plotting the
sum of squared error versus “K” numbers and wherever the slope kinks at
indicates the optimal number of clusters [@lee2022identifying].In
addition to the standard preprocessing steps, the Canopy algorithm may
also be utilized. The Canopy algorithm helps partion the data into
smaller subsets in which k-means clustering is performed on each
separate one to help reduce the computational complexity, and is best
used for large and complex data sets and enhances the fault tolerance
[@yuan2019research]. When it comes to smaller and less complex
data-sets, the other methods are better.

Once the amount of clusters has been determined, the quality of those
clusters can be assessed. The Calinski-Harabasz criterion is used to
determine the quality of the data and a higher value indicates the
clusters are better separated [@qabbaah2019using]. The original data set
is partitioned into clusters which have common traits and each cluster
has a cluster center which starts from seed points. Then the K-means
clustering method calculates the distance between each data point and
each cluster center and takes an nxd matrix and assigns the data
randomly to those clusters [@javadi2017classification]. The data points
are clustered by Euclidean distance of intensity values to the centroid,
and a new centroid is then calculated and the previous steps are
repeated until the labels of the clusters no longer change
[@dubey2013infected]. The Euclidean distance formula is used to compute
the distance between the object center and each centroid[@9320839].

One of the most important parts to make K-means clustering more
efficient is to find the most optimal centroids as quickly as possible.
A solution to this is to use principal component analysis to estimate
the initial centroids rather than random selection
[@zubair2022improved]. Basically this means, the process is looped over
and over until there is a convergence and there are distinct clusters.

### Applications of K-Means Clustering

There have been many useful applications of K-means clustering ranging
from color segmentation to identifying allergen severity by age group.
There has been research done on using color segmentation with K-means
clustering to identify the diseased parts of the fruit. This was
achieved by transforming the image from RGB to L\*a\*b\* and labeling
the pixels of the image via K-means clustering [@dubey2013infected].
Using this method, the diseased part of the fruit was picked out.
K-means clustering can also be used to cluster geographic locations.
Using an aquifer, K-means clustering was able to identify risk areas of
the aquifer without using fixed weights [@javadi2017classification].
When compared to data physically collected from the aquifer, it was
shown the map produced from clustering was very accurate. Another
example using geography is clustering regions of Indonesia, that was
based on trade and accommodation facilities. Regions of Indonesia were
clustered into three categories of high, medium, and low trade and
accommodation [@munawar2021cluster].

K-means clustering also has medical applications such as predicting
chemotherapy response to cancer, how skin changes as you age, pregnancy
problems, and how your allergen sensitivity changes as you age. Since it
doesn’t have the same limitations as traditional methods, K-means
clustering was chosen to predict chemotherapy response and was able to
create clusters that acted as bio-markers to determine the treatment’s
effectiveness [@nguyen2015prediction]. Based on different attributes of
the pores in one’s skin, K-means clustering was able to create groups
based on the detectability and size of the pore. From this information,
as well as the mean age of each cluster, the changes in skin were able
to be inferred by the age group of each cluster. Similarly, by using the
data from immunoglobulin tests, K-means clustering created groups based
on age to explain how allergen sensitivity actually decreased as age
increased [@zhao2023k]. Pregnancy problems can also be analyzed with
this method. K-means clustering was used to aid health professionals in
making decisions about interventions to reduce pregnancy problems
[@9154694].

K-means is also present in the tourism industry. By using a modified
K-means algorithm, a tourist’s preferences and behaviors can be
clustered, so the satisfaction and experience can be improved
[@ijgi13020040]. Additionally, K-means is useful in agriculture. When
using K-means clustering on soil, clusters were generated which improved
the understanding of soil classification and spatial representation of
the soil to make more informed decisions[@7377608].As a last example,
our method was used to cluster counties together based on similarity to
each other from economic factors to health behaviors, so county-level
counterfactuals could be found [@strutz2022determining]. 

## Variations of K-Means Clustering

Apart from the traditional method of K-means clustering, there is also
constrained K-means clustering. Constrained K-means clustering consists
of having lower bounds for clusters and the distances between those
clusters [@usami2014constrained]. This method either deletes or
consolidates clusters based off the constraints. Another variation of
the traditional method is the improved K-means clustering algorithm. The
improved algorithm uses two data structures to retain the cluster labels
and their distances, while reducing the amount of repeated distance
calculations [@na2010research]. This improved algorithm improves speed
and efficiency while maintaining accuracy. The next variation is called
the filtering algorithm. The filtering algorithm is an optimized
implementation of Lloyd's K-means using a kd-tree structure, and
performs best in clustering by refining candidate center selection
through axis-aligned splitting hyperplanes and efficient trimming
[@kanungo2002efficient]. The allure of this variation is it has great
efficiency, is simplistic, and is scalable to higher dimensions.
Genetic-based K-means (GBKM) clustering is another alternative offering
speed and accuracy. What makes this algorithm different from the
traditional one is the “centroids are encoded by chromosomes rather than
random initial cluster centroids”[@chougule2015genetic]. GBKM also
follows some of the same ideas of natural selection such as selection,
crossover, and mutation. This modified K-means algorithm selects the
fittest points from the clusters to produce a new generation by
performing single point crossover and one-bit flip mutation, until the
best results are found [@chougule2015genetic]. K-means clustering can
also be used in conjunction with other tools. An example is the use of
MapReduce from Hadoop to aid with the amount of data used. In this
process, two files are created where one contains the centroid values
and the other contains data point values and the mapper function is
coded to calculate the distance between each centroid and data point,
then assigned to the centroid with the lowest distance
[@anchalia2013mapreduce]. A new iteration of MapReduce will be performed
again until the centroid value is not updated. Lastly, there is the
extended K-means algorithm. Extended K-means takes the number of
elements within a cluster into consideration by sorting the clusters in
ascending order based on the number of elements in each cluster and
either recruits or drops elements from the first cluster [@6642738]. If
there are n elements in the cluster, and an element is needed, the
difference a-n, is calculated and points from other clusters are
recruited. 

## Limitations

Even though K-means clustering is a great tool for data analysis it
still has its limitations. A few of those limitations are the random
initializations of centroids, the requirement to know in advance the
number of clusters present within a data-set, and also the inability of
the algorithm to handle a wide variety of very different types of data
[@electronics9081295]. If you were to choose too many or too few
clusters, a lot of variability will be brought into the model. If the
data were to become more complex, there would be issues in the
clustering. There needs to be an adequate sample size and the data needs
to be pre-processed very carefully due to its scalability. 

## Methods

The methodology behind this analysis involves preprocessing the data,
picking variables of interest, determining the optimal number of
clusters, performing the k-means clustering, and identifying what each
cluster consists of. In order to make the data better suited for our
needs we will convert the Baptist and Mormon variables from a percentage
to a count using the "pop" variable. Afterwards a new dataframe
consisting variables of interest such as spirits, baptist, mormon,
fatal1517, fatal1820, fatal2124, and afatal, will be created and scaled.
In addition, Recursive Feature Elimination will be used to find
variables from the original dataframe to run our analysis on as well as
on our chosen variables. Overall we will be running k-means clustering
on three dataframes and collecting valuable insights from it. Next, the
Elbow method and Silhouette coefficient will be used to determine the
optimum number of clusters. The Silhouette Coefficient (equation 1) is
used to measure how similar a data point is to its own cluster compared
to other clusters based off a range of -1 to 1. A more positive values
means that the data is well placed within its cluster. In equation 1, a
is the average distance from data point to the other data points within
the same cluster and b is the minimum average distance from one point to
points in other clusters. The Elbow Method (equation 2) consists of
plotting the within-cluster sum of squares against the number of
clusters and visually selecting a point where where the rate of decrease
significantly slows down. In equation 2, k is the number of clusters, C
is the ith cluster, and \|\|x-mu\|\|\^2 is the squared Euclidean
distance. The Euclidean distance (equation 3) is the straight line
distance between two points where p and q are the points in an
n-dimensional space and n is the number of dimensions. Once the optimal
number of clusters is determined, k-means clustering will be performed
on each data frame and insights will be made based on the assignment of
clusters.

<center>

$$s=b-a/{max(a,b)}$$

Equation 1.

</center>

<center>

$$
WCSS(k)=\sum^k_{i=1}\sum_{x\in C_i}||x-\mu_i||^2
$$

Equation 2.

</center>

<center>

$$d(p,q)=\sqrt{\sum^n_{i=1}(q_i-p_i)^2}$$

Equation 3.

</center>

Here is an outlline of our overall method:

-   Preprocess the data

    -   Convert Baptist and Mormon variables to from percentage to count

    -   Check for NULL values

    -   Remove outliers

    -   scale the data

-   Picking variables of interest

    -   We will first create a new dataframe of variables of interest

    -   Create two other dataframes utilizing Recursive Feature
        Elimination

-   Optimal number of clusters

    -   The elbow method and Silhouette coefficient will be used to
        determine the number of clusters

-   Perform K-means clustering

    -   The kmeans function from the stats package will be used and the
        Hartigan-Wong alorgithm will be implemented

    -   The clustering will be visualized and the between SS/ total SS
        will be analyzed for the best fit

-   Interpretation of clusters

    -   A new clusters column is added to the original pre-scaled
        dataframe

    -   a summary table is created showcasing the average values within
        each cluster to make interpretation easier

## Analysis and Results

### Data and Visualization

For our fatal car crash clustering, we are using the dataset from
"Fatalities" from the AER R package which originally came from US
Department of Transportation Fatal Accident Reporting System and is
dated from 1982-1988. In addition, the income variable was obtained from
the US Bureau of Economic Analysis and unemployment rate was obtained
from the US Bureau of Labor Statistics. The data includes all the states
except Alaska and Hawaii.

| Variable     | Variable Type | Description                                                                                            |
|---------------|---------------|-------------------------------------------|
| state        | Categorical   | A categorical variable that represents the state                                                       |
| year         |               | The year is indicated by the categorical factor                                                        |
| spirits      | Continuous    | A numerical value indicating the amount of spirits consumed.                                           |
| unemp        | Continuous    | The unemployment rate expressed as a numerical value.                                                  |
| income       | Continuous    | A numerical value in 1987 dollars that represents the per capita personal income.                      |
| emppop       | Continuous    | The employment to population ratio expressed as a numerical value.                                     |
| beertax      | Continuous    | The tax on a case of beer expressed as a numerical value.                                              |
| baptist      | Continuous    | A number that indicates the proportion of Southern Baptists in the general population.                 |
| mormon       | Continuous    | A numerical value that indicates the proportion of Mormons in the general population.                  |
| drinkage     | Continuous    | A number that represents the minimum age at which alcohol is permitted.                                |
| dry          | Continuous    | A numerical number that indicates the proportion of people living in "dry" counties.                   |
| youngdrivers | Continuous    | A numerical value indicating the proportion of drivers between the ages of 15 and 24.                  |
| miles        | Continuous    | A numerical value indicating the average number of miles driven by each driver.                        |
| breath       | Boolean       | A categorical factor that denotes the existence of a law requiring a preliminary breath test.          |
| jail         | Boolean       | A categorical factor that indicates whether a jail sentence is required.                               |
| service      | Boolean       | A categorical factor that denotes the existence of a community service requirement.                    |
| fatal        | Continuous    | A number that indicates the overall number of people killed in cars.                                   |
| nfatal       | Continuous    | A numerical value that indicates the quantity of car deaths that occur at night.                       |
| sfatal       | Continuous    | A numerical value that indicates how many people have died in just one vehicle.                        |
| fatal1517    | Continuous    | A numerical value that indicates how many teenagers between the ages of 15 and 17 were killed in cars. |
| nfatal1517   | Continuous    | A numerical value that indicates how many teenagers, aged 15 to 17, are killed in cars at night.       |
| fatal1820    | Continuous    | A numerical value that indicates the total number of 18–20-year-olds who die in cars.                  |
| nfatal1820   | Continuous    | A numerical value that indicates how many teenagers, aged 18 to 20, are killed in cars at night.       |
| fatal2124    | Continuous    | A numerical value that indicates how many people aged 21 to 24 are killed in cars.                     |
| nfatal2124   | Continuous    | A numerical value that indicates how many people aged 21 to 24 are killed in cars.                     |
| afatal       | Continuous    | A numerical value that indicates the quantity of drunk driving deaths.                                 |
| pop          | Continuous    | A numerical figure that indicates the entire population.                                               |
| pop1517      | Continuous    | A numerical value that indicates the population of people aged 15 to 17.                               |
| pop1820      | Continuous    | A numerical value that denotes the 18–20 year old population.                                          |
| pop2124      | Continuous    | A numerical value that denotes the population of people aged 21 to 24.                                 |
| milestot     | Continuous    | A number (in millions) that indicates the total number of miles driven by a vehicle.                   |
| unemppopus   | Continuous    | The US unemployment rate expressed as a numerical value.                                               |
| Emppopus     | Continuous    | The US employment to population ratio expressed as a numerical value.                                  |
| gsp          | Continuous    | A numerical value that indicates the Gross State Product's (GSP) rate of change.                       |

```{r}
install.packages("tidyverse",repos = "http://cran.us.r-project.org")
install.packages("MASS", repos = "http://cran.us.r-project.org")
install.packages("fpc", repos = "http://cran.us.r-project.org")
install.packages("waterfalls", repos = "http://cran.us.r-project.org")
install.packages("gt", repos = "http://cran.us.r-project.org")
install.packages("gtable", repos = "http://cran.us.r-project.org")
install.packages("ggcorrplot", repos = "http://cran.us.r-project.org")
install.packages("ggplot2", repos = "http://cran.us.r-project.org")
install.packages("corrplot", repos = "http://cran.us.r-project.org")
install.packages("googlesheets4", repos = "http://cran.us.r-project.org")
install.packages("cluster", repos = "http://cran.us.r-project.org")
install.packages("factoextra", repos = "http://cran.us.r-project.org")
install.packages("DescTools", repos = "http://cran.us.r-project.org")
install.packages("isofor", repos = "http://cran.us.r-project.org")
install.packages("missForest", repos = "http://cran.us.r-project.org")
install.packages("AER", repos = "http://cran.us.r-project.org")
install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
library(AER)
library(e1071)
library(missForest)
library(DescTools)
library(MASS)
library(fpc)
library(cluster)
library(factoextra)
library(tidyverse)
library(waterfalls)
library(corrplot)
library(ggcorrplot)
library(googlesheets4) 
data("Fatalities")
df=Fatalities
df3=Fatalities
```

```{r}
summary(df)
```

This code block converts the percentage of Baptists and Mormons to a
count as well as picks the variables of interest such as spirits
consumption, the amount of baptist, the amount of mormons, the age range
of fatal car crashes, and alcohol related car crashes. In addition, we
check for any missing data

```{r}
df$baptist=df$baptist/100
df$mormon=df$mormon/100
df$baptist=df$baptist*df$pop
df$mormon=df$mormon*df$pop
df=df[c(3,8,9,20,22,24,26)]
sum(is.na(df))
```

```{r}
sum(is.null(df))
```

Here we scale the data and check for outliers. Although k-means is
sensitive to outliers, we will keep these outliers be we believe they
are not true outliers.

```{r}
df2=as.data.frame(scale(df))
boxplot(df2)
```

```{r}
detection=function(x) {
  q1=quantile(x,probs=.25) 
  q3=quantile(x,probs=.75) 
  iqr=q3-q1 
  x>q3+(iqr*1.5)|x<q1-(iqr*1.5)}

elimination= function(df,cols=names(df)) {
  for(col in cols) { 
    df=df[!detection(df[[col]]),]}
  return(df)} 
column_names=c('spirits','baptist','mormon', 'fatal1517','fatal1820', 'fatal2124', 'afatal')
cleaned_df2=elimination(df2,column_names)
boxplot(cleaned_df2)
```

In this code block we examine the correlation between each variable.

```{r}
model.matrix(~0+., data=cleaned_df2) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, type="lower", lab=TRUE, lab_size=1)

```

```{r}
summary(df2)
```

Here, we are checking for the optimal number of clusters using the elbow
method. According to this it is 7 clusters.

```{r}
fviz_nbclust(cleaned_df2, kmeans, method = "wss")
```

Here we are using the Silhouette method which shows 3 clusters.

```{r}
fviz_nbclust(cleaned_df2, kmeans, method = "silhouette")
```

For this next part we are using Recursive Feature Elimination to
determine which variables to keep based off a response variable. In this
case we are interested in the number of vehicle fatalities that involved
alcohol.

```{r}
sum(is.na(df3))
```

```{r}
df3=na.omit(df3)
```

```{r}
#We are only keeping numerical variables and only keeping the fatality variables related to age and alcohol since the other fatality variables are too similar. We are also getting rid of the population variables.
df3$baptist=df3$baptist/100
df3$mormon=df3$mormon/100
df3$baptist=df3$baptist*df3$pop
df3$mormon=df3$mormon*df3$pop
df3=df3[-c(1,2,14,15,16,17,18,19,21,23,25,27,28,29,30,31)]

control=rfeControl(functions = rfFuncs, method = "cv", number=10)
df3=as.data.frame(scale(df3))
predictors=df3[,-which(names(df3)=="afatal")]
response=df3$afatal
result=rfe(x=predictors, y = response, sizes=c(1:10), rfeControl = control, metric = "RMSE", method="cv")
print(result)
```

```{r}
df3=df3[c(6,7,12,13,14,15)]
boxplot(df3)
```

```{r}
column_names2=c('baptist','mormon', 'fatal1517','fatal1820', 'fatal2124', 'afatal')
cleaned_df3=elimination(df3,column_names2)
boxplot(cleaned_df3)
```

```{r}
model.matrix(~0+., data=cleaned_df3) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, type="lower", lab=TRUE, lab_size=1)
```

The elbow test for this new dataframe shows the optimal number of
clusters at 6.

```{r}
fviz_nbclust(cleaned_df3, kmeans, method = "wss")
```

The silhouette test for this new data frame shows 3 clusters as being
optimal.

```{r}
fviz_nbclust(cleaned_df3, kmeans, method = "silhouette")
```

We are looking for the best variables to use when considering all the
variables of interest.

```{r}
dfnew=as.data.frame(scale(df))
predictors=dfnew[,-which(names(dfnew)=="afatal")]
response=dfnew$afatal
result=rfe(x=predictors, y = response, sizes=c(1:10), rfeControl = control, metric = "RMSE", method="cv")
print(result)
```

We are checking for outliers in the new dataframe.

```{r}
dfnew=dfnew[-c(2)]
boxplot(dfnew)
```

Here we are removing outliers again.

```{r}
column_names3=c('spirits','mormon', 'fatal1517','fatal1820', 'fatal2124', 'afatal')
cleaned_dfnew=elimination(dfnew,column_names3)
boxplot(cleaned_dfnew)
```

Here is a correlation matrix showing the relationship between each
variable

```{r}
model.matrix(~0+., data=cleaned_dfnew) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, type="lower", lab=TRUE, lab_size=1)
```

In this graph, the optimal number of clusters is 6.

```{r}
fviz_nbclust(cleaned_dfnew, kmeans, method = "wss")
```

In this graph the optimal number of clusters is 4.

```{r}
fviz_nbclust(cleaned_dfnew, kmeans, method = "silhouette")
```

### Statistical Modeling

### Conclusion

## References
