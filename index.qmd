---
title: "Using K-Means Clustering to Determine Groupings of Fatal Car Crashes"
authors:
  - name: Kyle Knuth
  - name: Siva Gopavarapu
  - name: Nikhitha Karlapudi
  - name: Eswar Sai Burugupalley
  - name: Bala Venkataprasad Sadhana Vittal
date: 'today'
format:
  html:
    code-fold: False
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

[Slides](k-means clustering slides.html)

## Introduction

### What is "K-Means Clustering"?

K-means clustering is a form of unsupervised learning which uses “K”
clusters with assigned data to create groups. The main goal of K-means
clustering is to classify data within clusters, which are as similar as
possible, while also keeping it out of clusters as dissimilar as
possible [@sinaga2020unsupervised]. There are several methods of finding
the optimal number of clusters, such as the Elbow method, Silhouette
method, Gap statistic. The Silhouette method is used to visually find
the number of clusters by using the silhouette coefficients. The
silhouette coefficients can be between -1 and 1, where a value of 1
indicates the data is far from the neighboring cluster, a value of 0
means the data is close to the neighboring clusters and a value of -1
means the data was assigned to the wrong cluster [@shi2021quantitative].
The Gap statistic finds the optimal number of clusters by comparing the
within cluster dispersion changes to expected with a null dispersion
[@tibshirani2001estimating].  The Elbow method consists of plotting the
sum of squared error versus “K” numbers and wherever the slope kinks at
indicates the optimal number of clusters [@syakur2018integration].In
addition to the standard preprocessing steps, the Canopy algorithm may
also be utilized. The Canopy algorithm helps partion the data into
smaller subsets in which k-means clustering is performed on each
separate one to help reduce the computational complexity, and is best
used for large and complex data sets and enhances the fault tolerance
[@yuan2019research]. When it comes to smaller and less complex
data-sets, the other methods are better.

Once the amount of clusters has been determined, the quality of those
clusters can be assessed. The Calinski-Harabasz criterion is used to
determine the quality of the data and a higher value indicates the
clusters are better separated [@qabbaah2019using]. The original data set
is partitioned into clusters which have common traits and each cluster
has a cluster center which starts from seed points. Then the K-means
clustering method calculates the distance between each data point and
each cluster center and takes an nxd matrix and assigns the data
randomly to those clusters [@javadi2017classification]. The data points
are clustered by Euclidean distance of intensity values to the centroid,
and a new centroid is then calculated and the previous steps are
repeated until the labels of the clusters no longer change
[@dubey2013infected]. The Euclidean distance formula is used to compute
the distance between the object center and each centroid[@9320839].

One of the most important parts to make K-means clustering more
efficient is to find the most optimal centroids as quickly as possible.
A solution to this is to use principal component analysis to estimate
the initial centroids rather than random selection
[@zubair2022improved]. Basically this means, the process is looped over
and over until there is a convergence and there are distinct clusters.

### Applications of K-Means Clustering

There have been many useful applications of K-means clustering ranging
from color segmentation to identifying allergen severity by age group.
There has been research done on using color segmentation with K-means
clustering to identify the diseased parts of the fruit. This was
achieved by transforming the image from RGB to L\*a\*b\* and labeling
the pixels of the image via K-means clustering [@dubey2013infected].
Using this method, the diseased part of the fruit was picked out.
K-means clustering can also be used to cluster geographic locations.
Using an aquifer, K-means clustering was able to identify risk areas of
the aquifer without using fixed weights [@javadi2017classification].
When compared to data physically collected from the aquifer, it was
shown the map produced from clustering was very accurate. Another
example using geography is clustering regions of Indonesia, that was
based on trade and accommodation facilities. Regions of Indonesia were
clustered into three categories of high, medium, and low trade and
accommodation [@munawar2021cluster].

K-means clustering also has medical applications such as predicting
chemotherapy response to cancer, how skin changes as you age, pregnancy
problems, and how your allergen sensitivity changes as you age. Since it
doesn’t have the same limitations as traditional methods, K-means
clustering was chosen to predict chemotherapy response and was able to
create clusters that acted as bio-markers to determine the treatment’s
effectiveness [@nguyen2015prediction]. Based on different attributes of
the pores in one’s skin, K-means clustering was able to create groups
based on the detectability and size of the pore. From this information,
as well as the mean age of each cluster, the changes in skin were able
to be inferred by the age group of each cluster. Similarly, by using the
data from immunoglobulin tests, K-means clustering created groups based
on age to explain how allergen sensitivity actually decreased as age
increased [@zhao2023k]. Pregnancy problems can also be analyzed with
this method. K-means clustering was used to aid health professionals in
making decisions about interventions to reduce pregnancy problems
[@9154694].

K-means is also present in the tourism industry. By using a modified
K-means algorithm, a tourist’s preferences and behaviors can be
clustered, so the satisfaction and experience can be improved
[@ijgi13020040]. Additionally, K-means is useful in agriculture. When
using K-means clustering on soil, clusters were generated which improved
the understanding of soil classification and spatial representation of
the soil to make more informed decisions[@7377608].As a last example,
our method was used to cluster counties together based on similarity to
each other from economic factors to health behaviors, so county-level
counterfactuals could be found [@strutz2022determining]. 

## Variations of K-Means Clustering

Apart from the traditional method of K-means clustering, there is also
constrained K-means clustering. Constrained K-means clustering consists
of having lower bounds for clusters and the distances between those
clusters [@usami2014constrained]. This method either deletes or
consolidates clusters based off the constraints. Another variation of
the traditional method is the improved K-means clustering algorithm. The
improved algorithm uses two data structures to retain the cluster labels
and their distances, while reducing the amount of repeated distance
calculations [@na2010research]. This improved algorithm improves speed
and efficiency while maintaining accuracy. The next variation is called
the filtering algorithm. The filtering algorithm is an optimized
implementation of Lloyd's K-means using a kd-tree structure, and
performs best in clustering by refining candidate center selection
through axis-aligned splitting hyperplanes and efficient trimming
[@kanungo2002efficient]. The allure of this variation is it has great
efficiency, is simplistic, and is scalable to higher dimensions.
Genetic-based K-means (GBKM) clustering is another alternative offering
speed and accuracy. What makes this algorithm different from the
traditional one is the “centroids are encoded by chromosomes rather than
random initial cluster centroids”[@chougule2015genetic]. GBKM also
follows some of the same ideas of natural selection such as selection,
crossover, and mutation. This modified K-means algorithm selects the
fittest points from the clusters to produce a new generation by
performing single point crossover and one-bit flip mutation, until the
best results are found [@chougule2015genetic]. K-means clustering can
also be used in conjunction with other tools. An example is the use of
MapReduce from Hadoop to aid with the amount of data used. In this
process, two files are created where one contains the centroid values
and the other contains data point values and the mapper function is
coded to calculate the distance between each centroid and data point,
then assigned to the centroid with the lowest distance
[@anchalia2013mapreduce]. A new iteration of MapReduce will be performed
again until the centroid value is not updated. Lastly, there is the
extended K-means algorithm. Extended K-means takes the number of
elements within a cluster into consideration by sorting the clusters in
ascending order based on the number of elements in each cluster and
either recruits or drops elements from the first cluster [@6642738]. If
there are n elements in the cluster, and an element is needed, the
difference a-n, is calculated and points from other clusters are
recruited. 

## Limitations

Even though K-means clustering is a great tool for data analysis it
still has its limitations. A few of those limitations are the random
initializations of centroids, the requirement to know in advance the
number of clusters present within a data-set, and also the inability of
the algorithm to handle a wide variety of very different types of data
[@electronics9081295]. If you were to choose too many or too few
clusters, a lot of variability will be brought into the model. If the
data were to become more complex, there would be issues in the
clustering. There needs to be an adequate sample size and the data needs
to be pre-processed very carefully due to its scalability. 

## Methods

The methodology behind k-means clustering involves preprocessing the
data, picking variables of interest, determining the optimal number of
clusters, performing the k-means clustering, and identifying what each
cluster consists of. The data is typically preprocessed by first
removing any categorical variables, then removing outliers, and finally
performing feature scaling. Once the data has been preprocessed,
variables of interest will be picked and then the optimal number of
clusters will be determined. The first method of determining how many
clusters to use is the Silhouette Coefficient.  The Silhouette
Coefficient is used to measure how similar a data point is to its own
cluster compared to other clusters based off a range of -1 to 1 where a
more positive value means that the data is well placed within its
cluster[@shi2021quantitative]. The Silhouette Coefficient is shown below
in equation 1:

<center>$$S=b-a/max(b,\:a) \quad (1)$$</center>

S: Silhouette coefficient

a: the average distance from one data point to the other within the same
cluster

b: the minimum average distance from one point to points in other
clusters

The next method of determining how many clusters to use is the Elbow
method. The Elbow method consists of plotting the within-cluster sum of
squares against the number of clusters and visually selecting a point
where the rate of decrease significantly slows down
[@shi2021quantitative]. The Elbow method is shown below in equation 2:

<center>$$
WCSS=\sum^K_{k=1}\sum_{x_i=C_k}x_i-\mu^2_{k2} \quad (2)
$$</center>

WCSS: within-cluster sum of squares

x: data point in the cluster

C~K~: the current individual cluster

K: number of total clusters

$\mu_k$ : centroids corresponding to the cluster

The final method in determining how many clusters to use is the Gap
statistic. This method finds the optimal number of clusters by comparing
the within cluster dispersion changes to expected with a null
dispersion[@tibshirani2001estimating]. The Gap statistic is shown in
equation 3:

<center>$$
Gap_n(k)=E^*_n\{log(W_K)\}-log(W_k) \quad (3)
$$</center>

Gap~n~(k): Gap statistic

E\*n: expectation of sample size

n: sample size

k: number of clusters

W~K~:within-cluster sum of squares surrounding cluster means

The distance metric used for these methods is Euclidean distance.
Euclidean distance is defined as the straight line distance between two
points [@ultsch2022euclidean]. The Euclidean distance is shown in
equation 4:

<center>$$d(x,y)=\sqrt{\sum|x_i-y_i|^2} \quad (4)$$</center>

d(x,y): Euclidean distance

x: x-coordinate point

y: y-coordinate point

The actual method of k-means clustering is an iterative process
consisting of the following steps [@9320839]:

-   Determine the number of clusters

-   Randomly select an initial centroid

-   Using Euclidean distance, data is assigned to clusters in accordance
    to the distance of the centroids

-   A new centroid is recalculated based on the mean of the previous
    centroids.

-   All the previous steps are repeated until the mean of the centroids
    no longer changes

The assumptions of k-means clustering are as follows [@raykov2016k]:

-   The data is in a sphere around the centroid and each sphere has the
    same radius

-   The average of the points in a cluster is the centroid of the
    cluster meaning that outliers will dramatically affect clustering 

-   The density of each cluster is consistent

-   The number of clusters is known and fixed

## Analysis and Results

### Data Description

For our fatal car crash clustering, we are using the dataset from
"Fatalities" from the AER R package which originally came from US
Department of Transportation Fatal Accident Reporting System and is
dated from 1982-1988. In addition, the income variable was obtained from
the US Bureau of Economic Analysis and unemployment rate was obtained
from the US Bureau of Labor Statistics. The data includes all the states
except Alaska and Hawaii.

| Variable     | Variable Type | Description                                                                                            |
|------------|------------|------------------------------------------------|
| state        | Categorical   | A categorical variable that represents the state                                                       |
| year         | Categorical   | The year is indicated by the categorical factor                                                        |
| spirits      | Continuous    | A numerical value indicating the amount of spirits consumed.                                           |
| unemp        | Continuous    | The unemployment rate expressed as a numerical value.                                                  |
| income       | Continuous    | A numerical value in dollars that represents the per capita personal income.                           |
| emppop       | Continuous    | The employment to population ratio expressed as a numerical value.                                     |
| beertax      | Continuous    | The tax on a case of beer expressed as a numerical value.                                              |
| baptist      | Continuous    | A number that indicates the proportion of Southern Baptists in the general population.                 |
| mormon       | Continuous    | A numerical value that indicates the proportion of Mormons in the general population.                  |
| drinkage     | Continuous    | A number that represents the minimum age at which alcohol is permitted.                                |
| dry          | Continuous    | A numerical number that indicates the proportion of people living in "dry" counties.                   |
| youngdrivers | Continuous    | A numerical value indicating the proportion of drivers between the ages of 15 and 24.                  |
| miles        | Continuous    | A numerical value indicating the average number of miles driven by each driver.                        |
| breath       | Boolean       | A categorical factor that denotes the existence of a law requiring a preliminary breath test.          |
| jail         | Boolean       | A categorical factor that indicates whether a jail sentence is required.                               |
| service      | Boolean       | A categorical factor that denotes the existence of a community service requirement.                    |
| fatal        | Continuous    | A number that indicates the overall number of people killed in cars.                                   |
| nfatal       | Continuous    | A numerical value that indicates the quantity of car deaths that occur at night.                       |
| sfatal       | Continuous    | A numerical value that indicates how many people have died in just one vehicle.                        |
| fatal1517    | Continuous    | A numerical value that indicates how many teenagers between the ages of 15 and 17 were killed in cars. |
| nfatal1517   | Continuous    | A numerical value that indicates how many teenagers, aged 15 to 17, are killed in cars at night.       |
| fatal1820    | Continuous    | A numerical value that indicates the total number of 18–20-year-olds who die in cars.                  |
| nfatal1820   | Continuous    | A numerical value that indicates how many teenagers, aged 18 to 20, are killed in cars at night.       |
| fatal2124    | Continuous    | A numerical value that indicates how many people aged 21 to 24 are killed in cars.                     |
| nfatal2124   | Continuous    | A numerical value that indicates how many people aged 21 to 24 are killed in cars.                     |
| afatal       | Continuous    | A numerical value that indicates the quantity of drunk driving deaths.                                 |
| pop          | Continuous    | A numerical figure that indicates the entire population.                                               |
| pop1517      | Continuous    | A numerical value that indicates the population of people aged 15 to 17.                               |
| pop1820      | Continuous    | A numerical value that denotes the 18–20 year old population.                                          |
| pop2124      | Continuous    | A numerical value that denotes the population of people aged 21 to 24.                                 |
| milestot     | Continuous    | A number (in millions) that indicates the total number of miles driven by a vehicle.                   |
| unemppopus   | Continuous    | The US unemployment rate expressed as a numerical value.                                               |
| Emppopus     | Continuous    | The US employment to population ratio expressed as a numerical value.                                  |
| gsp          | Continuous    | A numerical value that indicates the Gross State Product's (GSP) rate of change.                       |

# Data analysis (Preprocessing and cleaning)

Install necessary packages to perform data cleaning, analysis,
visualization, and model building.

```{r}
library(maps)
library(mapproj)
library(psych)
library(caret)
library(AER)
library(e1071)
library(missForest)
library(MASS)
library(fpc)
library(cluster)
library(factoextra)
library(tidyverse)
library(waterfalls)
library(corrplot)
library(ggcorrplot)
library(googlesheets4) 
data("Fatalities")
df1=Fatalities
df=Fatalities
df3=Fatalities
```

```{r}
head(df)
```

We plan to conduct our analysis on a chosen dataset as shown below. We
convert the percentage of Baptists and Mormons to a count as well as
pick our variables of interest such as spirits consumption, the amount
of Baptists, the amount of Mormons, the amount the alcohol is taxed, and
alcohol related car crashes. These variables were chosen to observe the
relationship between alcohol consumption and religion in regards to
fatal car crashes.

```{r}
df$baptist=df$baptist/100
df$mormon=df$mormon/100
df$baptist=df$baptist*df$pop
df$mormon=df$mormon*df$pop
df=df[c(3,7,8,9,26)]
describe(df)
```

We also check to see if there are any missing or null values in our data
and there does not appear to be any.

```{r}
print(c(sum(is.na(df)),sum(is.null(df))))
```

The five graphs below show a boxplot of each variable of interest. As
seen below there are some outliers that need to be removed so we can
cluster our data effectively.

```{r}
boxplot(df$spirits, horizontal = TRUE, xlab="Count", ylab="Spirits Consumed")
```

```{r}
boxplot(df$beertax, horizontal = TRUE, xlab="Count", ylab="Beertax")
```

```{r}
boxplot(df$baptist, horizontal = TRUE, xlab="Count", ylab="Baptist")
```

```{r}
boxplot(df$mormon, horizontal = TRUE, xlab="Count", ylab="Mormon")
```

```{r}
boxplot(df$afatal, horizontal = TRUE, xlab="Count", ylab="Fatal Car Crashes (Alcohol)")
```

Next, we first check if there are any outliers in our data and remove
them before scaling the data. We need to remove the outliers since it
can affect the centroid in the k-means clsutering process. In addition,
the data needs to be scaled since k-means clustering is reliant on
Euclidean distance and a large difference in values can adversly affect
it.

```{r}
detection=function(x) {
  q1=quantile(x,probs=.25) 
  q3=quantile(x,probs=.75) 
  iqr=q3-q1 
  x>q3+(iqr*1.5)|x<q1-(iqr*1.5)}

elimination= function(df,cols=names(df)) {
  for(col in cols) { 
    df=df[!detection(df[[col]]),]}
  return(df)} 
column_names=c('spirits','baptist','mormon', 'beertax', 'afatal')
df2=df
cleaned_df_2=elimination(df2,column_names)
cleaned_df2=as.data.frame(scale(cleaned_df_2))
boxplot(cleaned_df2, ylab="Count")
```

### Identifying the optimal number of clusters

The next step in our analysis is to figure how many clusters to use. The
elbow method is performed and as shown by the graph, the optimal number
of clusters is 6.

```{r}
fviz_nbclust(cleaned_df2, kmeans, method = "wss")
```

Another method to determine the optimal number of clusters is finding
the Silhouette Coefficient. According to this graph the optimal number
of clusters could be either 10 or 6. These numbers were chosen since
they are the closest to 1 for average silhouette width.

```{r}
fviz_nbclust(cleaned_df2, kmeans, method = "silhouette")
```

Lastly we are using the gap statistic as a final check to determine the
best number of clusters. This graph also shows 6 clusters as being the
most optimal.

```{r}
fviz_nbclust(cleaned_df2, kmeans, method = "gap_stat")
```

For this first dataframe, all three methods seem to agree that the
optimal number of clusters to use is 6.

### Modeling With K-Means Clustering

Now we are using our selected number of clusters (6) for our dataframe
to perform k-means clustering. The plot below shows clustering for our
dataframe consisting of our variables of interest.

```{r}
custom_colors=c("red","green", "yellow", "orange", "blue", "cyan")
km <- kmeans(cleaned_df2, centers= 6, nstart = 25)
fviz_cluster(km,data=cleaned_df2, geom = "point")+scale_color_manual(values = custom_colors)+scale_fill_manual(values = custom_colors)

```

Using between sum of squares divided by the total sum of squares as a
metric of best fit for our clusters, our first k-means clustering
analysis shows 71.4%

```{r}
km
```

# Results

After performing k-means clustering a summary table is created to show
the average characteristics of each cluster

```{r}
cleaned_df_2$cluster=km$cluster
clustertable=cleaned_df_2%>%
  group_by(cluster) %>%
  summarise_all("mean")
print(clustertable)
```

```{r}
cleaned_df2$cluster=km$cluster
clustertable2=cleaned_df2%>%
  group_by(cluster) %>%
  summarise_all("mean")
print(clustertable2)
```

### Cluster analysis (average)

The standardized data will be used to make fair comparisons regarding
the religion variables

Cluster 1:

-   consists of average of 1.47 alcoholic drinks consumed per person

-   There was an average of 323 alcohol related car crashes

-   The average beertax was 93 cents

-   This cluster consisted of mainly Baptists rather than Mormons

Cluster 2:

-   Consists of an average of 1.72 alcoholic drinks consumed per person

-   There was an average of 117 alcohol related car crashes

-   The average beertax was 23.7 cents

-   This cluster consists of somewhat more Mormons than Baptists

Cluster 3:

-   Consists of an average of 1.39 alcoholic drinks consumed per person

-   There was an average of 113 alcohol related car crashes

-   The average beertax was 50.1 cents

-   This cluster consists had similar amounts of Baptists and Mormons

Cluster 4:

-   Consists of an average of 2.18 alcoholic drinks consumed per person

-   There was an average of 142 alcohol related car crashes

-   The average beertax was 24.5 cents

-   This cluster consists had similar amounts of Baptists and Mormons

Cluster 5:

-   Consists of an average of 1.6 alcoholic drinks consumed per person

-   There was an average of 473 alcohol related car crashes

-   The average beertax was 27 cents

-   This cluster contains more Mormons than Baptists

Cluster 6:

-   Consists of an average of 1.24 alcoholic drinks consumed per person

-   There was an average of 295 alcohol related car crashes

-   The average beertax was 35 cents

-   This cluster contains significantly more Baptists than Mormons

### Cluster analysis (by variable)

In this first chart it is shown that cluster 5 had the highest number of
car crash fatalities related to alcohol while clusters 2 and 3 had the
lowest amount. Cluster 1 had the second highest amount.

```{r}
y=clustertable2$cluster
x=clustertable2$afatal
data=as.data.frame(x)
data$y=y

ggplot(data, aes(x = y, y = x)) +
  geom_bar(stat = "identity", fill = custom_colors) + 
  labs(x = "Cluster", y = "Afatal")
```

In this next chart, cluster 4 had the highest number of alcohol consumed
while cluster 6 had the lowest amount. The second highest was cluster 2.

```{r}
y=clustertable2$cluster
x=clustertable2$spirits
data=as.data.frame(x)
data$y=y

ggplot(data, aes(x = y, y = x)) +
  geom_bar(stat = "identity", fill = custom_colors) + 
  labs(x = "Cluster", y = "Spirits")

```

As shown below, clusters 1 and 6 had the highest population of Baptists
while clusters 3 and 4 had the lowest amount.

```{r}
y=clustertable2$cluster
x=clustertable2$baptist
data=as.data.frame(x)
data$y=y

ggplot(data, aes(x = y, y = x)) +
  geom_bar(stat = "identity", fill = custom_colors) + 
  labs(x = "Cluster", y = "Baptist")

```

In this chart, cluster 2 had the highest population of Mormons and
clusters 3 and 4 had the lowest population.

```{r}
y=clustertable2$cluster
x=clustertable2$mormon
data=as.data.frame(x)
data$y=y

ggplot(data, aes(x = y, y = x)) +
  geom_bar(stat = "identity", fill = custom_colors) + 
  labs(x = "Cluster", y = "Mormon")

```

Cluster 1 had the highest beertax while clusters 2 and 4 had the lowest
beertax.

```{r}
y=clustertable2$cluster
x=clustertable2$beertax
data=as.data.frame(x)
data$y=y

ggplot(data, aes(x = y, y = x)) +
  geom_bar(stat = "identity", fill = custom_colors) + 
  labs(x = "Cluster", y = "Beertax")

```

### Cluster Analysis (State)

The below graphs show the same information as the bar charts but at the
state level. We created a subset of the original data that now has the
clusters from our k-means clustering analyis to make inferences. This is
to highlight which states belong to which cluster.

```{r}
dfsub=Fatalities
subsetdf=dfsub[rownames(dfsub) %in% rownames(cleaned_df_2),]
subsetdf$cluster=km$cluster
subsetdf$cluster=as.factor(subsetdf$cluster)
custom_colors=c("red", "green", "yellow", "orange", "blue", "cyan", "purple")
ggplot(subsetdf, aes(x=state, afatal, color=cluster)) + geom_point()+ scale_color_manual(values = custom_colors)
```

```{r}
ggplot(subsetdf, aes(x=state, mormon, color=cluster)) + geom_point()+ scale_color_manual(values = custom_colors)
```

```{r}
ggplot(subsetdf, aes(x=state, baptist, color=cluster)) + geom_point()+ scale_color_manual(values = custom_colors)
```

```{r}
ggplot(subsetdf, aes(x=state, spirits, color=cluster)) + geom_point()+ scale_color_manual(values = custom_colors)
```

```{r}
ggplot(subsetdf, aes(x=state, beertax, color=cluster)) + geom_point()+ scale_color_manual(values = custom_colors)
```

The map below shows a better depiction of the clustered states:

```{r}
map_data=map_data("state")

state_colors=c("florida"="red",
               "louisiana"="red",
               "mississippi"="red",
               "north Carolina"="red",
               "oklahoma"="red",
               "virginia"="red",
               "arkansas"="cyan",
               "kentucky"="cyan",
               "missouri"="cyan",
               "tennessee"="cyan",
               "colorado"="green", 
               "montana"="green",
               "new mexico"="green",
               "wyoming"="green",
               "connecticut"="orange",
               "delaware"="orange",
               "maryland"="orange",
               "massachusetts"="orange",
               "minnesota"="orange",
               "new jersey"="orange",
               "north dakota"="yellow",
               "rhode island"="orange",
               "vermont"="orange",
               "wisconsin"="orange",
               "illinois"="blue",
               "indiana"="blue",
               "michigan"="blue",
               "new york"="blue",
               "ohio"="blue",
               "pennsylvania"="blue",
               "iowa"="yellow",
               "kansas"="yellow",
               "maine"="yellow",
               "nebraska"="yellow",
               "south dakota"="yellow",
               "west virginia"="yellow")



map_data$color=ifelse(map_data$region %in%
                        names(state_colors),
                      state_colors[map_data$region], NA)

map_data$Cluster=ifelse(map_data$color=="red", "Cluster 1",
                        ifelse(map_data$color=="green","Cluster 2",
                               ifelse(map_data$color=="yellow", "Cluster 3",
                                      ifelse(map_data$color=="orange","Cluster 4",
                                             ifelse(map_data$color=="blue", "Cluster 5",
                                                    ifelse(map_data$color=="cyan","Cluster 6",NA))))))


custom_colors=c("Cluster 1"="red", "Cluster 2"="green", "Cluster 3"="yellow", "Cluster 4"="orange", "Cluster 5"="blue", "Cluster 6"="cyan")


ggplot(map_data,aes(x=long,y=lat,group=group, fill=Cluster))+geom_polygon(color="black", size=0.25)+theme_void()+scale_fill_manual(values = custom_colors, na.value="white")




```

As shown by the map, the clustered states seem form 6 different
sub-regions of the U.S. based off our chosen variables. Cluster 1
appears to contain the South. Cluster 2 seems to be the West. Cluster 3
is in the center of the U.S. with two states residing further East.
Cluster 4 is primarily in the Northeast of the U.S. Cluster 5 is in the
Great Lakes and MidEast area. Lastly, Cluster 6 is in the
Eastern-Central part of the U.S.

### Conclusion

In conclusion, we have analyzed data from the "Fatalities" dataset,
which contains information on fatal car crashes from the US Department
of Transportation's Fatal Accident Reporting System. As shown by our
analysis, K-means clustering is a great option to group data together to
generate useful insights.  We have shown how the analysis was performed
such as pre-processing the data, determining the optimal number of
clusters as well as creating and implementing our K-means clustering
model. We generated 6 clusters to explain trends regarding alcohol
related car crashes related to alcohol consumption, religion, and beer
tax. From this analysis, we were able to assign these clusters to the
rest of the data and create subregions of the United States, each with
its own characteristics, and make inferences. These inferences can help
provide an understanding of how each region handles alcohol and its
impacts. Using k-means clustering, we were able to obtain important
insights into the complex interactions between these variables, which
could possible help identify possible areas for focused intervention.

In addition, we recognize that choosing the correct number clusters can
be difficult as well as very subjective. Due to this, there can be
varying results and accuracy. For example, interpreting our graphs of
the Elbow method and Silhouette method was somewhat subjective, but the
decision became easier after observing the Gap Statistic.

## References
